{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping in Python\n",
    "\n",
    "It might not be a bit surprising, but the internet is an interesting source of data for aspiring data scientists! Even more so for acquiring textual data. Hence the need for skills to retrieve data from the internet.\n",
    "\n",
    "The act of using a computer program to retrieve web pages and store and analyse their content is known as 'web scraping'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Law, copyright and ethics\n",
    "This might be a good time to point out that, while web scraping is not in itself an illegal activity, there are some finer points which you will have to consider before starting to apply your skills.\n",
    "\n",
    "Firstly, while reading web pages on the internet is fine, it is obviously what they're there for, depending on what you want to do with the acquired data might be illegal. You will need to consider both privacy laws and copyright claims that might rest upon said data. If, for instance, you're using the output of a famous AI chatbot to train your own chatbot, let's call it SeepDeek, the owner of the famous AI chatbot might want to have a word with you about your disregard of their legal terms for using their service.\n",
    "\n",
    "Secondly, computer programs are usually way better at consuming enormous amounts of web pages than their human counterparts. Reading a web page and navigating a web site 'by hand' is obviously going to be slow in comparison with the automated version. A computer program downloading web pages might in fact cause a congestion on the computer program serving the web pages, the web server. This would result in other users of the same web service experiencing slow downs. When used in a malicious sense this would be regarded as a denial-of-service attack (a DOS attack).  \n",
    "It is obviously unethical to burden a web server to a point where other users start experiencing such slowdowns. And besides, the proprietor of this web server is likely charged monthly by both CPU usage and used bandwidth, which you are both increasing solely for your own benefit.   \n",
    "\n",
    "That's why you really should take all the necessary care to ensure you are allowed to read a web site using web scraping techniques. To help both you and the proprietor of the web server (and/or the data being served) come to a sensible agreement, you could start by reading up on a standard called '[robots.txt](https://en.wikipedia.org/wiki/Robots.txt)'. This name refers to a file the proprietor can put on the web server. The contents of the file try to explain whether and, if so, how you're allowed to read the contents of the web site.\n",
    "\n",
    "As always, it is completely up to you to be an nice person. But [you really should](https://en.wikipedia.org/wiki/Karma)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web pages - what are they made of?\n",
    "\n",
    "Before you start downloading all kinds of web pages with Python, it might be worth your while to read up on what web pages are made actually of.\n",
    "\n",
    "Most web pages are made of [HTML](https://en.wikipedia.org/wiki/HTML), a markup language.  \n",
    "And as the [internet](https://en.wikipedia.org/wiki/History_of_the_World_Wide_Web) grew beyond the ideas of Sir Tim Berners-Lee, people invented stuff like [CSS](https://en.wikipedia.org/wiki/CSS) to style their web pages and [JavaScript](https://en.wikipedia.org/wiki/JavaScript) to allow the pages to become more interactive.  \n",
    "You can find tutorials about all of these standards on [w3schools.com](https://www.w3schools.com/html/).\n",
    "\n",
    "And when scraping for data you might also encounter [JSON](https://en.wikipedia.org/wiki/JSON), which is a quite common data format nowadays. Python even has a [built-in library](https://www.w3schools.com/python/python_json.asp) to convert to and from JSON text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading a single web page in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is always the case, a lot of options are available for downloading stuff from the internet using Python.  \n",
    "Let's start simple: downloading single web pages.\n",
    "\n",
    "Python has a built-in package for working with web addresses. These addresses, designating a resource located somewhere on the internet, are called [Uniform Resource Locator](https://en.wikipedia.org/wiki/URL)s or URLs for short. The package is aptly named [urllib](https://docs.python.org/3/library/urllib.request.html).\n",
    "\n",
    "Using the urllib.request.urlopen() will return a file handle from which you can read the web page.  \n",
    "Downloading a web page is therefor as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html>\\n\\n<html lang=\"en\" data-content_root=\"../\">\\n  <head>\\n    <meta charset=\"utf-8\" />\\n    '\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = 'https://docs.python.org/3/library/urllib.request.html#urllib.request.urlopen'\n",
    "with urllib.request.urlopen(url) as f:\n",
    "    html = f.read()\n",
    "\n",
    "print(html[:100])  ## byte string cut short for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the example above, the urlopen() function returns a byte string by default.  \n",
    "If you prefer a Python str (string) to work with, you can decode the byte string using an appropriate codepage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "\n",
      "<html lang=\"en\" data-content_root=\"../\">\n",
      "  <head>\n",
      "    <meta charset=\"utf-8\" />\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "html = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "print(html[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading (all) tables from a web page\n",
    "\n",
    "The pandas library has a function you will find useful to read all tables from a web page in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tables read: 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>NOC</th>\n",
       "      <th>Gold</th>\n",
       "      <th>Silver</th>\n",
       "      <th>Bronze</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Norway</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>United States</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Italy*</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Japan</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Germany</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>France</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Slovenia</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Bulgaria</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Canada</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>China</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Totals (15 entries)</td>\n",
       "      <td>Totals (15 entries)</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Rank                  NOC  Gold  Silver  Bronze  Total\n",
       "0                     1               Norway     3       1       2      6\n",
       "1                     2        United States     2       0       0      2\n",
       "2                     3               Italy*     1       2       6      9\n",
       "3                     4                Japan     1       2       1      4\n",
       "4                     5              Austria     1       2       0      3\n",
       "5                     6              Germany     1       1       1      3\n",
       "6                     7       Czech Republic     1       1       0      2\n",
       "7                     7               France     1       1       0      2\n",
       "8                     7               Sweden     1       1       0      2\n",
       "9                    10          Switzerland     1       0       0      1\n",
       "10                   11             Slovenia     0       1       0      1\n",
       "11                   11          South Korea     0       1       0      1\n",
       "12                   13             Bulgaria     0       0       1      1\n",
       "13                   13               Canada     0       0       1      1\n",
       "14                   13                China     0       0       1      1\n",
       "15  Totals (15 entries)  Totals (15 entries)    13      13      13     39"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import StringIO\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/2026_Winter_Olympics_medal_table'\n",
    "# wikipedia will reject requests from the default urllib user agent,\n",
    "# so we replace it with an alternative\n",
    "request = urllib.request.Request(url, headers={'User-Agent': 'Mozilla'})\n",
    "tables = pd.read_html(\n",
    "    StringIO(\n",
    "        urllib.request.urlopen(request).read().decode('utf-8')\n",
    "    )\n",
    ")\n",
    "print('Number of tables read:', len(tables))\n",
    "tables[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that it will return all tables in the web page, even the ones that aren't easily or even actually visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading web pages and more\n",
    "\n",
    "While the built-in urllib package is useful for accidental single page downloads, more complicated scenarios require a more able web client. As you could read on the Python documentation page for the [urllib.request](https://docs.python.org/3/library/urllib.request.html#module-urllib.request) module, the [requests package](https://requests.readthedocs.io/en/latest/), a third-party library, provides a commonly used higher-level interface for interacting with web servers using HTTP.  \n",
    "It will, for instance, allow you to mimic other browsers, authenticate to web sites or create more complicated HTTP requests like GET and POST requests carrying data.\n",
    "\n",
    "The requests package is a part of the Anaconda standard package, so you're likely to have it installed.\n",
    "\n",
    "The rather elaborate example below uses Eindhoven's [Open Data API](https://data.eindhoven.nl/pages/home/) to download and show recent (realtime) measurements of particulate matter for the location of the TU/e campus.  \n",
    "\n",
    "If you want to learn more about using requests, you might find some tutorials on Youtube:\n",
    "- [BeautifulSoup + Requests | Web Scraping in Python](https://www.youtube.com/watch?v=bargNl2WeN4)\n",
    "- follow-up: [Find and Find_All | Web Scraping in Python](https://www.youtube.com/watch?v=xjA1HjvmoMY)\n",
    "- follow-up: [Scraping Data from a Real Website | Web Scraping in Python](https://www.youtube.com/watch?v=8dTpNajxaH0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation count: 2\n",
      "obs 01 - 2026-02-08T23:40:00+01:00 - pm25 = 17.39\n",
      "obs 02 - 2026-02-08T23:30:01+01:00 - pm25 = 17.24\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'https://data.eindhoven.nl/api/explore/v2.1/catalog/datasets/real-time-fijnstof-monitoring/records'\n",
    "# 'select=pm25&where=&limit=20'\n",
    "params = {\n",
    "    'select': 'lossetimestamps_timestamp,pm25',\n",
    "    'where': 'latitude=\"51.4454\" AND longitude=\"5.4854\"',\n",
    "    'timezone': 'Europe/Amsterdam',\n",
    "    'limit': 20,\n",
    "}\n",
    "response = requests.get(url, params)\n",
    "if response.status_code == 200:\n",
    "    data = json.loads(response.text)\n",
    "    print(f'Observation count: {data[\"total_count\"]}')\n",
    "    for i, observation in enumerate(data['results'], 1):\n",
    "        print(f'obs {i:02} - {observation[\"lossetimestamps_timestamp\"]} - pm25 = {observation[\"pm25\"]}')\n",
    "else:\n",
    "    print('Error retrieving specified URL:')\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Navigating a web site and extracting parts from web pages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While downloading full web pages is a breeze in Python, retrieving useful information from them is a different matter.  \n",
    "Like natural language, HTML, the markup language in which web pages are written, is subject to change over time. And to make matters worse, web developers change the layout and markup of their web sites every so often. Which causes your scripts to grind to a halt, spewing errors about missing tags you assumed would always be there.  \n",
    "\n",
    "You could start by using regular expressions to extract information from webpages. They are just text, so REs will work fine.\n",
    "\n",
    "For example, let's retrieve today's featured article from Wikipedia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featured article: Master Juba\n",
      "Hyperlink: https://en.wikipedia.org/wiki/Master_Juba\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "response = requests.get(url, headers={'User-Agent': 'Python Requests'})\n",
    "try:\n",
    "    html = response.text\n",
    "    m = re.search(\n",
    "        r'<a href=\"(?P<rel_url>[^\"]+)\" title=\"(?P<title>[^\"]+)\">Full&#160;article...</a>',\n",
    "        html\n",
    "    )\n",
    "    assert m is not None, 'Link for Featured Article not found!'\n",
    "    rel_url = m.group('rel_url')\n",
    "    title = m.group('title')\n",
    "    print(f'Featured article: {title}')\n",
    "    print(f'Hyperlink: https://en.wikipedia.org{rel_url}')\n",
    "except Exception as ex:\n",
    "    print('Error retrieving featured article from Wikipedia!')\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a higher-level look at the contents of the web page, you could use a library like [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/).  \n",
    "It turns the HTML for the web page into a tree-like structure, allowing you to search for specific tags, or tags with specific attributes or content: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master Juba (c. 1825 – c. 1854) was an African-American dancer. He was one of the first black performers in the United States to play onstage for white audiences and the only one of the era to tour with a white minstrel group. He began his career in Manhattan's Five Points neighborhood and moved to minstrel shows in the mid-1840s. His act featured a sequence in which he imitated famous dancers, then closed by performing in his style. In 1848, he became a sensation in Britain for his dance style, but writers treated him as an exhibit on display. Juba's popularity faded and he died around 1854, probably of fever. He was largely forgotten by historians until a 1947 article by Marian Hannah Winter popularized his story. Juba's dancing style was percussive, varied in tempo and expressive. It likely incorporated European folk steps and African-derived steps used by plantation slaves. Juba was highly influential in the development of tap, jazz, and step-dancing styles. (Full article...)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "response = requests.get(url, headers={'User-Agent': 'Python Requests'})\n",
    "html = BeautifulSoup(response.text)\n",
    "\n",
    "tfa_div = html.find('div', {'id': 'mp-tfa'})\n",
    "tfa_p = tfa_div.find('p')\n",
    "print(tfa_p.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automating web scraping and beyond...\n",
    "\n",
    "Without going into further details, it is worth mentioning, if only for the sake of completeness, that specific solutions exist to automate advanced web scraping solutions. One such solution using Python is [scrapy](https://scrapy.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
