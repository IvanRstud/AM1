{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping in Python\n",
    "\n",
    "It might not be a bit surprising, but the internet is an interesting source of data for aspiring data scientists! Even more so for acquiring textual data. Hence the need for skills to retrieve data from the internet.\n",
    "\n",
    "The act of using a computer program to retrieve web pages and store and analyse their content is known as 'web scraping'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Law, copyright and ethics\n",
    "This might be a good time to point out that, while web scraping is not in itself an illegal activity, there are some finer points which you will have to consider before starting to apply your skills.\n",
    "\n",
    "Firstly, while reading web pages on the internet is fine, it is obviously what they're there for, depending on what you want to do with the acquired data might be illegal. You will need to consider both privacy laws and copyright claims that might rest upon said data. If, for instance, you're using the output of a famous AI chatbot to train your own chatbot, let's call it SeepDeek, the owner of the famous AI chatbot might want to have a word with you about your disregard of their legal terms for using their service.\n",
    "\n",
    "Secondly, computer programs are usually way better at consuming enormous amounts of web pages than their human counterparts. Reading a web page and navigating a web site 'by hand' is obviously going to be slow in comparison with the automated version. A computer program downloading web pages might in fact cause a congestion on the computer program serving the web pages, the web server. This would result in other users of the same web service experiencing slow downs. When used in a malicious sense this would be regarded as a denial-of-service attack (a DOS attack).  \n",
    "It is obviously unethical to burden a web server to a point where other users start experiencing such slowdowns. And besides, the proprietor of this web server is likely charged monthly by both CPU usage and used bandwidth, which you are both increasing solely for your own benefit.   \n",
    "\n",
    "That's why you really should take all the necessary care to ensure you are allowed to read a web site using web scraping techniques. To help both you and the proprietor of the web server (and/or the data being served) come to a sensible agreement, you could start by reading up on a standard called '[robots.txt](https://en.wikipedia.org/wiki/Robots.txt)'. This name refers to a file the proprietor can put on the web server. The contents of the file try to explain whether and, if so, how you're allowed to read the contents of the web site.\n",
    "\n",
    "As always, it is completely up to you to be an nice person. But [you really should](https://en.wikipedia.org/wiki/Karma)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web pages - what are they made of?\n",
    "\n",
    "Before you start downloading all kinds of web pages with Python, it might be worth your while to read up on what web pages are made actually of.\n",
    "\n",
    "Most web pages are made of [HTML](https://en.wikipedia.org/wiki/HTML), a markup language.  \n",
    "And as the [internet](https://en.wikipedia.org/wiki/History_of_the_World_Wide_Web) grew beyond the ideas of Sir Tim Berners-Lee, people invented stuff like [CSS](https://en.wikipedia.org/wiki/CSS) to style their web pages and [JavaScript](https://en.wikipedia.org/wiki/JavaScript) to allow the pages to become more interactive.  \n",
    "You can find tutorials about all of these standards on [w3schools.com](https://www.w3schools.com/html/).\n",
    "\n",
    "And when scraping for data you might also encounter [JSON](https://en.wikipedia.org/wiki/JSON), which is a quite common data format nowadays. Python even has a [built-in library](https://www.w3schools.com/python/python_json.asp) to convert to and from JSON text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading a single web page in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is always the case, a lot of options are available for downloading stuff from the internet using Python.  \n",
    "Let's start simple: downloading single web pages.\n",
    "\n",
    "Python has a built-in package for working with web addresses. These addresses, designating a resource located somewhere on the internet, are called [Uniform Resource Locator](https://en.wikipedia.org/wiki/URL)s or URLs for short. The package is aptly named [urllib](https://docs.python.org/3/library/urllib.request.html).\n",
    "\n",
    "Using the urllib.request.urlopen() will return a file handle from which you can read the web page.  \n",
    "Downloading a web page is therefor as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html>\\n\\n<html lang=\"en\" data-content_root=\"../\">\\n  <head>\\n    <meta charset=\"utf-8\" />\\n    '\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = 'https://docs.python.org/3/library/urllib.request.html#urllib.request.urlopen'\n",
    "with urllib.request.urlopen(url) as f:\n",
    "    html = f.read()\n",
    "\n",
    "print(html[:100])  ## byte string cut short for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the example above, the urlopen() function returns a byte string by default.  \n",
    "If you prefer a Python str (string) to work with, you can decode the byte string using an appropriate codepage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "\n",
      "<html lang=\"en\" data-content_root=\"../\">\n",
      "  <head>\n",
      "    <meta charset=\"utf-8\" />\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "html = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "print(html[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading (all) tables from a web page\n",
    "\n",
    "The pandas library has a function you will find useful to read all tables from a web page in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tables read: 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>.mw-parser-output .tooltip-dotted{border-bottom:1px dotted;cursor:help}NOC</th>\n",
       "      <th>Gold</th>\n",
       "      <th>Silver</th>\n",
       "      <th>Bronze</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Norway</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Italy*</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>United States</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Austria</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Germany</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>France</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Japan</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Australia</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Great Britain</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Slovenia</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>Kazakhstan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>China</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Poland</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Latvia</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Finland</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Bulgaria</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Totals (26 entries)</td>\n",
       "      <td>Totals (26 entries)</td>\n",
       "      <td>74</td>\n",
       "      <td>76</td>\n",
       "      <td>73</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Rank  \\\n",
       "0                     1   \n",
       "1                     2   \n",
       "2                     3   \n",
       "3                     4   \n",
       "4                     5   \n",
       "5                     6   \n",
       "6                     7   \n",
       "7                     8   \n",
       "8                     9   \n",
       "9                    10   \n",
       "10                   11   \n",
       "11                   12   \n",
       "12                   13   \n",
       "13                   14   \n",
       "14                   15   \n",
       "15                   16   \n",
       "16                   17   \n",
       "17                   17   \n",
       "18                   19   \n",
       "19                   20   \n",
       "20                   21   \n",
       "21                   21   \n",
       "22                   23   \n",
       "23                   24   \n",
       "24                   25   \n",
       "25                   26   \n",
       "26  Totals (26 entries)   \n",
       "\n",
       "   .mw-parser-output .tooltip-dotted{border-bottom:1px dotted;cursor:help}NOC  \\\n",
       "0                                              Norway                           \n",
       "1                                              Italy*                           \n",
       "2                                       United States                           \n",
       "3                                         Netherlands                           \n",
       "4                                             Austria                           \n",
       "5                                              Sweden                           \n",
       "6                                         Switzerland                           \n",
       "7                                             Germany                           \n",
       "8                                              France                           \n",
       "9                                               Japan                           \n",
       "10                                          Australia                           \n",
       "11                                      Great Britain                           \n",
       "12                                             Canada                           \n",
       "13                                     Czech Republic                           \n",
       "14                                           Slovenia                           \n",
       "15                                        South Korea                           \n",
       "16                                             Brazil                           \n",
       "17                                         Kazakhstan                           \n",
       "18                                              China                           \n",
       "19                                             Poland                           \n",
       "20                                             Latvia                           \n",
       "21                                        New Zealand                           \n",
       "22                                            Georgia                           \n",
       "23                                            Finland                           \n",
       "24                                           Bulgaria                           \n",
       "25                                            Belgium                           \n",
       "26                                Totals (26 entries)                           \n",
       "\n",
       "    Gold  Silver  Bronze  Total  \n",
       "0     12       7       9     28  \n",
       "1      8       4      11     23  \n",
       "2      6       8       5     19  \n",
       "3      6       5       1     12  \n",
       "4      5       7       3     15  \n",
       "5      5       5       1     11  \n",
       "6      5       2       3     10  \n",
       "7      4       7       6     17  \n",
       "8      4       7       4     15  \n",
       "9      4       5       9     18  \n",
       "10     3       1       1      5  \n",
       "11     3       0       0      3  \n",
       "12     2       4       5     11  \n",
       "13     2       2       0      4  \n",
       "14     2       1       1      4  \n",
       "15     1       2       3      6  \n",
       "16     1       0       0      1  \n",
       "17     1       0       0      1  \n",
       "18     0       3       2      5  \n",
       "19     0       3       1      4  \n",
       "20     0       1       1      2  \n",
       "21     0       1       1      2  \n",
       "22     0       1       0      1  \n",
       "23     0       0       3      3  \n",
       "24     0       0       2      2  \n",
       "25     0       0       1      1  \n",
       "26    74      76      73    223  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import StringIO\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/2026_Winter_Olympics_medal_table'\n",
    "# wikipedia will reject requests from the default urllib user agent,\n",
    "# so we replace it with an alternative\n",
    "request = urllib.request.Request(url, headers={'User-Agent': 'Mozilla'})\n",
    "tables = pd.read_html(\n",
    "    StringIO(\n",
    "        urllib.request.urlopen(request).read().decode('utf-8')\n",
    "    )\n",
    ")\n",
    "print('Number of tables read:', len(tables))\n",
    "tables[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that it will return all tables in the web page, even the ones that aren't easily or even actually visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading web pages and more\n",
    "\n",
    "While the built-in urllib package is useful for accidental single page downloads, more complicated scenarios require a more able web client. As you could read on the Python documentation page for the [urllib.request](https://docs.python.org/3/library/urllib.request.html#module-urllib.request) module, the [requests package](https://requests.readthedocs.io/en/latest/), a third-party library, provides a commonly used higher-level interface for interacting with web servers using HTTP.  \n",
    "It will, for instance, allow you to mimic other browsers, authenticate to web sites or create more complicated HTTP requests like GET and POST requests carrying data.\n",
    "\n",
    "The requests package is a part of the Anaconda standard package, so you're likely to have it installed.\n",
    "\n",
    "The rather elaborate example below uses Eindhoven's [Open Data API](https://data.eindhoven.nl/pages/home/) to download and show recent (realtime) measurements of particulate matter for the location of the TU/e campus.  \n",
    "\n",
    "If you want to learn more about using requests, you might find some tutorials on Youtube:\n",
    "- [BeautifulSoup + Requests | Web Scraping in Python](https://www.youtube.com/watch?v=bargNl2WeN4)\n",
    "- follow-up: [Find and Find_All | Web Scraping in Python](https://www.youtube.com/watch?v=xjA1HjvmoMY)\n",
    "- follow-up: [Scraping Data from a Real Website | Web Scraping in Python](https://www.youtube.com/watch?v=8dTpNajxaH0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation count: 10\n",
      "obs 01 - 2026-02-17T10:40:00+01:00 - pm25 = 4.77\n",
      "obs 02 - 2026-02-17T10:50:00+01:00 - pm25 = 7.23\n",
      "obs 03 - 2026-02-17T11:00:00+01:00 - pm25 = 9.34\n",
      "obs 04 - 2026-02-17T11:10:01+01:00 - pm25 = 10.79\n",
      "obs 05 - 2026-02-17T11:20:00+01:00 - pm25 = 9.74\n",
      "obs 06 - 2026-02-17T10:30:00+01:00 - pm25 = 7.54\n",
      "obs 07 - 2026-02-17T12:00:00+01:00 - pm25 = 8.27\n",
      "obs 08 - 2026-02-17T11:30:00+01:00 - pm25 = 7.44\n",
      "obs 09 - 2026-02-17T11:40:00+01:00 - pm25 = 5.96\n",
      "obs 10 - 2026-02-17T11:50:01+01:00 - pm25 = 7.96\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'https://data.eindhoven.nl/api/explore/v2.1/catalog/datasets/real-time-fijnstof-monitoring/records'\n",
    "# 'select=pm25&where=&limit=20'\n",
    "params = {\n",
    "    'select': 'lossetimestamps_timestamp,pm25',\n",
    "    'where': 'latitude=\"51.4454\" AND longitude=\"5.4854\"',\n",
    "    'timezone': 'Europe/Amsterdam',\n",
    "    'limit': 20,\n",
    "}\n",
    "response = requests.get(url, params)\n",
    "if response.status_code == 200:\n",
    "    data = json.loads(response.text)\n",
    "    print(f'Observation count: {data[\"total_count\"]}')\n",
    "    for i, observation in enumerate(data['results'], 1):\n",
    "        print(f'obs {i:02} - {observation[\"lossetimestamps_timestamp\"]} - pm25 = {observation[\"pm25\"]}')\n",
    "else:\n",
    "    print('Error retrieving specified URL:')\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Navigating a web site and extracting parts from web pages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While downloading full web pages is a breeze in Python, retrieving useful information from them is a different matter.  \n",
    "Like natural language, HTML, the markup language in which web pages are written, is subject to change over time. And to make matters worse, web developers change the layout and markup of their web sites every so often. Which causes your scripts to grind to a halt, spewing errors about missing tags you assumed would always be there.  \n",
    "\n",
    "You could start by using regular expressions to extract information from webpages. They are just text, so REs will work fine.\n",
    "\n",
    "For example, let's retrieve today's featured article from Wikipedia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featured article: USS Romeo\n",
      "Hyperlink: https://en.wikipedia.org/wiki/USS_Romeo\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "response = requests.get(url, headers={'User-Agent': 'Python Requests'})\n",
    "try:\n",
    "    html = response.text\n",
    "    m = re.search(\n",
    "        r'<a href=\"(?P<rel_url>[^\"]+)\" title=\"(?P<title>[^\"]+)\">Full&#160;article...</a>',\n",
    "        html\n",
    "    )\n",
    "    assert m is not None, 'Link for Featured Article not found!'\n",
    "    rel_url = m.group('rel_url')\n",
    "    title = m.group('title')\n",
    "    print(f'Featured article: {title}')\n",
    "    print(f'Hyperlink: https://en.wikipedia.org{rel_url}')\n",
    "except Exception as ex:\n",
    "    print('Error retrieving featured article from Wikipedia!')\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a higher-level look at the contents of the web page, you could use a library like [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/).  \n",
    "It turns the HTML for the web page into a tree-like structure, allowing you to search for specific tags, or tags with specific attributes or content: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USS Romeo was a sternwheel steamer that saw service as a tinclad warship during the American Civil War. Completed in August 1862 as a civilian merchant vessel, she was instead purchased by the Union Navy in October. Commissioned in December, she cleared naval mines from the Yazoo River later that month before participating in operations against Confederate-held Fort Hindman in January 1863. Romeo was involved in some of the operations of the Vicksburg campaign in 1863, particularly the Yazoo Pass expedition. Later in 1863, Romeo was involved in part of the Little Rock campaign. During February 1864, she was part of an expedition up the Yazoo River to Yazoo City, Mississippi. She spent most of the rest of the war patrolling the Mississippi River, encountering Confederate land forces on several occasions. Romeo was declared surplus at the end of the war and was sold to civilian owners. Eventually converted into a sidewheel steamer, Romeo ceased to appear in the shipping registers in 1870. (Full article...)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "response = requests.get(url, headers={'User-Agent': 'Python Requests'})\n",
    "html = BeautifulSoup(response.text)\n",
    "\n",
    "tfa_div = html.find('div', {'id': 'mp-tfa'})\n",
    "tfa_p = tfa_div.find('p')\n",
    "print(tfa_p.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automating web scraping and beyond...\n",
    "\n",
    "Without going into further details, it is worth mentioning, if only for the sake of completeness, that specific solutions exist to automate advanced web scraping solutions. One such solution using Python is [scrapy](https://scrapy.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
