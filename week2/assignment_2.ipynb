{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "folder = Path(\"/Users/ivan/Desktop/Fontys/AML1/AM1-1/data/texts\") # write import path/reader yourself as on macOS pathes \n",
    "#looks different than on Windows, so generalised path doesnt work properly lol \n",
    "\n",
    "books = []\n",
    "\n",
    "for file in folder.glob(\"*.txt\"):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()      # read whole file as one string\n",
    "        books.append(text)   # add entire book as ONE string\n",
    "\n",
    "print(len(books))           # number of books\n",
    "print(type(books[0]))       # should be <class 'str'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿The Project Gutenberg eBook of The Complete Works of William Shakespeare\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the country where you are located\n",
      "before using this eBook.\n",
      "\n",
      "Title: The Complete Works of William Shakespeare\n",
      "\n",
      "Author: William Shakespeare\n",
      "\n",
      "Release date: January 1, 1994 [eBook #100]\n",
      "                Most recently updated: August 24, 2025\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK THE COMPLETE WORKS OF WILLIAM SHAKESPEARE ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Complete Works of William Shakespeare\n",
      "\n",
      "by William Shakespeare\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                    Contents\n",
      "\n",
      "    THE SONNETS\n",
      "    ALLâS WELL THAT ENDS WELL\n",
      "    THE TRAGEDY OF ANTONY AND CLEOPAT\n",
      "gutenberg.org/donate.\n",
      "\n",
      "Section 5. General Information About Project Gutenbergâ¢ electronic works\n",
      "\n",
      "Professor Michael S. Hart was the originator of the Project\n",
      "Gutenbergâ¢ concept of a library of electronic works that could be\n",
      "freely shared with anyone. For forty years, he produced and\n",
      "distributed Project Gutenbergâ¢ eBooks with only a loose network of\n",
      "volunteer support.\n",
      "\n",
      "Project Gutenbergâ¢ eBooks are often created from several printed\n",
      "editions, all of which are confirmed as not protected by copyright in\n",
      "the U.S. unless a copyright notice is included. Thus, we do not\n",
      "necessarily keep eBooks in compliance with any particular paper\n",
      "edition.\n",
      "\n",
      "Most people start at our website which has the main PG search\n",
      "facility: www.gutenberg.org.\n",
      "\n",
      "This website includes information about Project Gutenbergâ¢,\n",
      "including how to make donations to the Project Gutenberg Literary\n",
      "Archive Foundation, how to help produce our new eBooks, and how to\n",
      "subscribe to our email newsletter to hear about new eBooks.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cleaning the text\n",
    "print(books[0][:1000])\n",
    "print(books[0][-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_RE = re.compile(r\"\\*\\*\\*\\s*START OF (?:THE )?PROJECT GUTENBERG EBOOK.*?\\*\\*\\*\", re.IGNORECASE | re.DOTALL)\n",
    "END_RE   = re.compile(r\"\\*\\*\\*\\s*END OF (?:THE )?PROJECT GUTENBERG EBOOK.*?\\*\\*\\*\", re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "def clean_gutenberg(text: str) -> str:\n",
    "    # 1) remove header before START marker - 1st pattern\n",
    "    m = START_RE.search(text)\n",
    "    if m:\n",
    "        text = text[m.end():]\n",
    "\n",
    "    # 2) remove footer after END marker (license etc.) - 2nd pattern\n",
    "    m = END_RE.search(text)\n",
    "    if m:\n",
    "        text = text[:m.start()]\n",
    "\n",
    "    # 3) remove common structural patterns that bias stats\n",
    "    # 3a) chapter headings (roman numerals + digits)\n",
    "    text = re.sub(r\"(?im)^\\s*chapter\\s+([ivxlcdm]+|\\d+)\\b.*$\", \"\", text)\n",
    "\n",
    "    # 3b) collapse excessive blank lines\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "cleaned_books = [clean_gutenberg(b) for b in books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Complete Works of William Shakespeare\n",
      "\n",
      "by William Shakespeare\n",
      "\n",
      "                    Contents\n",
      "\n",
      "    THE SONNETS\n",
      "    ALLâS WELL THAT ENDS WELL\n",
      "    THE TRAGEDY OF ANTONY AND CLEOPATRA\n",
      "    AS YOU LIKE IT\n",
      "    THE COMEDY OF ERRORS\n",
      "    THE TRAGEDY OF CORIOLANUS\n",
      "    CYMBELINE\n",
      "    THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\n",
      "    THE FIRST PART OF KING HENRY THE FOURTH\n",
      "    THE SECOND PART OF KING HENRY THE FOURTH\n",
      "    THE LIFE OF KING HENRY THE FIFTH\n",
      "    THE FIRST PART OF HENRY THE SIXTH\n",
      "    THE SECOND PART OF KING HENRY THE SIXTH\n",
      "    THE THIRD PART OF KING HENRY THE SIXTH\n",
      "    KING HENRY THE EIGHTH\n",
      "    THE LIFE AND DEATH OF KING JOHN\n",
      "    THE TRAGEDY OF JULIUS CAESAR\n",
      "    THE TRAGEDY OF KING LEAR\n",
      "    LOVEâS LABOURâS LOST\n",
      "    THE TRAGEDY OF MACBETH\n",
      "    MEASURE FOR MEASURE\n",
      "    THE MERCHANT OF VENICE\n",
      "    THE MERRY WIVES OF WINDSOR\n",
      "    A MIDSUMMER NIGHTâS DREAM\n",
      "    MUCH ADO ABOUT NOTHING\n",
      "    THE TRAGEDY OF OTHELLO, THE MOOR OF VENICE\n",
      "    PERICLES, PRINCE OF TYRE\n",
      "    KING RICHARD THE SECOND\n",
      "    KI\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_books[0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tokens = []\n",
    "for text in cleaned_books:\n",
    "    raw_tokens.extend(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 13758303\n",
      "Unique tokens: 472496\n",
      "Top 20 tokens: [('the', 660536), ('and', 422476), ('of', 390691), ('to', 353059), ('a', 256963), ('in', 222829), ('I', 193344), ('was', 152970), ('that', 150253), ('he', 141079), ('his', 140785), ('with', 112474), ('for', 94901), ('as', 93690), ('had', 92086), ('is', 91305), ('it', 88995), ('not', 85752), ('at', 81835), ('you', 81206)]\n"
     ]
    }
   ],
   "source": [
    "counter = Counter(raw_tokens)\n",
    "\n",
    "print(\"Total tokens:\", len(raw_tokens))\n",
    "print(\"Unique tokens:\", len(counter))\n",
    "print(\"Top 20 tokens:\", counter.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ex2 - BPE tokenizer\n",
    "from tokenizers import Tokenizer # main object for tokenization\n",
    "from tokenizers.models import BPE # model implementing Byte Pair Encoding.\n",
    "from tokenizers.trainers import BpeTrainer # trains the BPE vocabulary\n",
    "from tokenizers.pre_tokenizers import Whitespace # simple pre-tokenizer splitting on spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpre_tokenizer \u001b[38;5;241m=\u001b[39m Whitespace() \n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m BpeTrainer(vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m, min_frequency\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, special_tokens\u001b[38;5;241m=\u001b[39m[], end_of_word_suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</w>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mtrain_from_iterator(cleaned_books, trainer\u001b[38;5;241m=\u001b[39mtrainer)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE())  # BPE model with unknown token\n",
    "tokenizer.pre_tokenizer = Whitespace() \n",
    "trainer = BpeTrainer(vocab_size=20000, min_frequency=1, special_tokens=[], end_of_word_suffix=\"</w>\")\n",
    "tokenizer.train_from_iterator(cleaned_books, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"bpe_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all cleaned books into one corpus\n",
    "all_text = \" \".join(cleaned_books)\n",
    "\n",
    "# Encode entire corpus\n",
    "encoding = tokenizer.encode(all_text)\n",
    "\n",
    "# Save token IDs\n",
    "with open(\"bpe_token_ids.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\" \".join(map(str, encoding.ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens after lemmatization: 403563\n",
      "Top 20 lemmatized tokens:\n",
      "[('the', 712257), ('be', 453824), ('and', 452491), ('of', 397187), ('to', 362566), ('a', 358006), ('in', 234945), ('i', 193367), ('have', 182058), ('he', 177997), ('that', 158162), ('his', 148902), ('it', 117746), ('with', 116791), ('for', 102795), ('you', 89562), ('not', 88061), ('at', 87536), ('my', 84312), ('her', 77033)]\n"
     ]
    }
   ],
   "source": [
    "#ex3 \n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 1) Tokenize all cleaned books into ONE token list (same approach as notebook)\n",
    "raw_tokens = []\n",
    "for text in cleaned_books:\n",
    "    raw_tokens.extend(text.split())\n",
    "\n",
    "# Optional but recommended: filter out pure punctuation/numbers\n",
    "# Keeps tokens like \"dog\" and removes things like \"—\" or \"123\"\n",
    "raw_tokens = [t for t in raw_tokens if any(ch.isalpha() for ch in t)]\n",
    "\n",
    "# 2) POS tagging (this can take time on many books)\n",
    "pos_tags = nltk.pos_tag(raw_tokens)\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default\n",
    "\n",
    "# 3) Lemmatize with lowercasing FIRST (as assignment says)\n",
    "lemmatized_tokens = [\n",
    "    lemmatizer.lemmatize(word.lower(), pos=get_wordnet_pos(tag))\n",
    "    for word, tag in pos_tags\n",
    "]\n",
    "\n",
    "# 4) Count + report\n",
    "lemma_counter = Counter(lemmatized_tokens)\n",
    "\n",
    "print(len(lemma_counter))\n",
    "print(lemma_counter.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ivan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384297\n",
      "[('the', 712257), ('and', 452495), ('of', 397193), ('to', 362566), ('a', 268463), ('in', 234945), ('i', 193366), ('he', 178013), ('that', 158198), ('wa', 154077), ('hi', 148905), ('it', 117746), ('with', 116791), ('for', 102795), ('as', 101508), ('is', 93971), ('had', 92996), ('you', 89564), ('not', 88079), ('at', 87536)]\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "# 1) Tokenize all cleaned books into ONE token list\n",
    "raw_tokens = []\n",
    "for text in cleaned_books:\n",
    "    raw_tokens.extend(text.split())\n",
    "\n",
    "# Optional but recommended: keep only tokens that contain letters\n",
    "raw_tokens = [t for t in raw_tokens if any(ch.isalpha() for ch in t)]\n",
    "\n",
    "# 2) Lowercase, then stem\n",
    "stemmed_tokens = [stemmer.stem(token.lower()) for token in raw_tokens]\n",
    "\n",
    "# 3) Count + report\n",
    "stem_counter = Counter(stemmed_tokens)\n",
    "\n",
    "print(len(stem_counter))\n",
    "print(stem_counter.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "less tokens + \"be\" is not in the list etc\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
