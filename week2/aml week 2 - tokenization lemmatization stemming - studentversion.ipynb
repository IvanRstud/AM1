{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f9dcb1a",
   "metadata": {},
   "source": [
    "# Tokenization, Lemmatization and Stemming\n",
    "\n",
    "In this notebook, we want to learn about three important terms in NLP: Tokenization, Lemmatization and Stemming.\n",
    "\n",
    "In summary:\n",
    "1. Tokenization: Splitting text into smaller units (tokens). (happiness -> ['hap', 'pi', 'ness'])\n",
    "2. Stemming: Cutting words down to a crude root form. (happiness -> happi)\n",
    "3. Lemmatization: Reducing words to their dictionary base form. (is -> be)\n",
    "\n",
    "The pipeline is typically: raw text → tokenization → (optional) stemming/lemmatization → further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d995e312",
   "metadata": {},
   "source": [
    "We start by loading the books that you donwloaded during your first assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3988560f",
   "metadata": {},
   "source": [
    "We get a list of `n_books` books:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5aa2bbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "folder = Path(\"/Users/ivan/Desktop/Fontys/AML1/AM1-1/data/texts\")\n",
    "\n",
    "books = []\n",
    "\n",
    "for file in folder.glob(\"*.txt\"):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()      # read whole file as one string\n",
    "        books.append(text)   # add entire book as ONE string\n",
    "\n",
    "print(len(books))           # number of books\n",
    "print(type(books[0]))       # should be <class 'str'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee613690",
   "metadata": {},
   "source": [
    "Let's first create a list of all words in the books via `.split()`. That's a very simple and naive way to tokenize text, which just splits the text on spaces.\n",
    "Note, this is often not enough because it doesn’t handle punctuation, contractions, special characters, or subword structures, which are important for accurate NLP processing. That's why we look into the BPE tokenizer later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e8ca6f2-7fb9-4259-bc1d-7018eef1cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tokens = []\n",
    "for text in books:\n",
    "    raw_tokens.extend(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cad66b66-4368-4e77-94b5-813f662772b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ï»¿The',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'eBook',\n",
       " 'of',\n",
       " 'The',\n",
       " 'Complete',\n",
       " 'Works',\n",
       " 'of',\n",
       " 'William',\n",
       " 'Shakespeare',\n",
       " 'This',\n",
       " 'ebook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b03c7be",
   "metadata": {},
   "source": [
    "Let's compute some statistics on `raw_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "707a0c37-d68b-4ba0-838b-55aaedc1e5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 14060521\n",
      "Unique tokens: 473240\n",
      "Top 20 tokens: [('the', 676599), ('and', 428709), ('of', 401425), ('to', 359893), ('a', 262346), ('in', 228362), ('I', 193406), ('was', 153163), ('that', 151392), ('he', 141269), ('his', 140808), ('with', 116834), ('for', 96995), ('as', 94545), ('is', 93395), ('had', 92089), ('it', 89942), ('not', 88200), ('you', 86376), ('at', 83091)]\n"
     ]
    }
   ],
   "source": [
    "counter = Counter(raw_tokens)\n",
    "\n",
    "print(\"Total tokens:\", len(raw_tokens))\n",
    "print(\"Unique tokens:\", len(counter))\n",
    "print(\"Top 20 tokens:\", counter.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e2d48",
   "metadata": {},
   "source": [
    "Note that the statistic is case-sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa7e3bbf-7e4a-487d-aa2c-e4c3431f1362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50696"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter['The']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4858d327",
   "metadata": {},
   "source": [
    "To deal with the case sensitivity, we can first call `.lower()` on each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a72666f2-e52a-47e4-981a-a104915f834a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens before: 473240\n",
      "Unique tokens after: 428190\n",
      "Before:\n",
      "676599 50696 3498\n",
      "After:\n",
      "730793\n"
     ]
    }
   ],
   "source": [
    "lower_tokens = [t.lower() for t in raw_tokens]\n",
    "lower_counter = Counter(lower_tokens)\n",
    "\n",
    "print(\"Unique tokens before:\", len(counter))\n",
    "print(\"Unique tokens after:\", len(lower_counter))\n",
    "\n",
    "print(\"Before:\")\n",
    "print(counter[\"the\"], counter[\"The\"], counter[\"THE\"])\n",
    "\n",
    "print(\"After:\")\n",
    "print(lower_counter[\"the\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d67159",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48614da",
   "metadata": {},
   "source": [
    "Tokenization is the process of splitting text into smaller units called tokens, such as words, subwords, or characters.\n",
    "It is usually the first step in an NLP pipeline, turning raw text into pieces that a computer can process and analyze.\n",
    "Modern systems often use subword tokenization (like BPE) so they can handle new or rare words by breaking them into meaningful parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac3b91a",
   "metadata": {},
   "source": [
    "We start by introducting BPE (Byte Pair Encoding): A subword tokenization method that iteratively merges the most frequent pairs of characters or symbols to build a compact, reusable vocabulary. In the following we learn how to tokenize a text with BPE in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db39ac24",
   "metadata": {},
   "source": [
    "We use the tokenizers library from Hugging Face, which implements BPE efficiently in Rust and exposes it in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6d1f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer # main object for tokenization\n",
    "from tokenizers.models import BPE # model implementing Byte Pair Encoding.\n",
    "from tokenizers.trainers import BpeTrainer # trains the BPE vocabulary\n",
    "from tokenizers.pre_tokenizers import Whitespace # simple pre-tokenizer splitting on spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af22f9b5",
   "metadata": {},
   "source": [
    "Instead of applying BPE immediately to the whole text lets first focus on a small example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4defa146",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"low lower lowest newer wider best\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aee909",
   "metadata": {},
   "source": [
    "Initialize a BPE Tokenizer first. Pre-tokenization ensures BPE merges operate inside words rather than on raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42c63acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE())  # BPE model with unknown token\n",
    "tokenizer.pre_tokenizer = Whitespace()         # Split text by spaces first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb9edd8",
   "metadata": {},
   "source": [
    "Train the tokenizer\n",
    "- The trainer counts frequent symbol pairs and merges them iteratively.\n",
    "- vocab_size limits the number of subword tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8484fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = BpeTrainer(vocab_size=23, min_frequency=1, special_tokens=[])\n",
    "tokenizer.train_from_iterator(corpus, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba0acc5",
   "metadata": {},
   "source": [
    "Let's see what the the create vocabulary (collection of all tokens) looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80a8b188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'est': 15,\n",
       " 'low': 13,\n",
       " 'l': 4,\n",
       " 'lo': 12,\n",
       " 'r': 7,\n",
       " 'wider': 21,\n",
       " 's': 8,\n",
       " 'd': 1,\n",
       " 'der': 17,\n",
       " 'ider': 19,\n",
       " 'lower': 22,\n",
       " 'es': 14,\n",
       " 'w': 10,\n",
       " 'i': 3,\n",
       " 'n': 5,\n",
       " 'e': 2,\n",
       " 't': 9,\n",
       " 'er': 11,\n",
       " 'o': 6,\n",
       " 'best': 16,\n",
       " 'new': 20,\n",
       " 'b': 0,\n",
       " 'ew': 18}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120991e4",
   "metadata": {},
   "source": [
    "You see that the word 'lowest' is not included in this vocabulary. Let's see how it's split into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd105f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['low', 'est']\n",
      "IDs: [13, 15]\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(\"lowest\")\n",
    "print(\"Tokens:\", output.tokens)\n",
    "print(\"IDs:\", output.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b20c629",
   "metadata": {},
   "source": [
    "Lets continue by tokenizing our books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c69abcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokens: ['lowest']\n",
      "Tokens: ['bath', 'room']\n",
      "Tokens: ['avail', 'ability']\n",
      "Tokens: ['hee', 'll', 'lo', 'oo', 'oo']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE())  # BPE model with unknown token\n",
    "tokenizer.pre_tokenizer = Whitespace() \n",
    "trainer = BpeTrainer(vocab_size=20000, min_frequency=1, special_tokens=[])\n",
    "tokenizer.train_from_iterator(books, trainer=trainer)\n",
    "\n",
    "for w in ['lowest', 'bathroom', 'availability', 'heelllooooo']:\n",
    "    output = tokenizer.encode(w)\n",
    "    print(\"Tokens:\", output.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54bc4e7",
   "metadata": {},
   "source": [
    "We can also reverse the encoding operationg by going back from ids to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49661613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bath', 'room']\n",
      "bath room\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.encode('bathroom').ids\n",
    "print(tokenizer.encode('bathroom').tokens)\n",
    "print(tokenizer.decode(ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641a4552",
   "metadata": {},
   "source": [
    "Note that encoding the word bathroom is still split into two words, the tokenizer does not know anymore that it had originally no space in between them. That information was lost during training. \n",
    "\n",
    "BPE tokenizers used in real LLMs add a word-boundary symbol such as:\n",
    "-\t`</w>` (classic BPE, as in the textbook)\n",
    "-\t`Ġ` (used by GPT-2 / RoBERTa)\n",
    "\n",
    "These markers preserve whether a token started with or without a space, making decoding lossless. To achieve this we can for example add `end_of_word_suffix=\"</w>\"`to the `BpeTrainer`. Now we introduced a new character that symbolizes the end of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f79efb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "['ba', 'th', 'room</w>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bathroom'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE())  # BPE model with unknown token\n",
    "tokenizer.pre_tokenizer = Whitespace() \n",
    "trainer = BpeTrainer(vocab_size=20000, min_frequency=1, special_tokens=[], end_of_word_suffix=\"</w>\")\n",
    "tokenizer.train_from_iterator(books, trainer=trainer)\n",
    "print(tokenizer.encode('bathroom').tokens)\n",
    "\n",
    "from tokenizers.decoders import BPEDecoder\n",
    "tokenizer.decoder = BPEDecoder(suffix=\"</w>\")\n",
    "ids = tokenizer.encode('bathroom').ids\n",
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff88d100",
   "metadata": {},
   "source": [
    "Now let's see how we can save a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "734f3178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This</w>', 'is</w>', 'a</w>', 'test</w>']\n"
     ]
    }
   ],
   "source": [
    "# Save tokenizer to a file\n",
    "tokenizer.save(\"bpe_tokenizer.json\")\n",
    "\n",
    "# Load tokenizer from file\n",
    "tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
    "\n",
    "# Now you can use your tokenizer as before\n",
    "output = tokenizer.encode(\"This is a test\")\n",
    "print(output.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7d354",
   "metadata": {},
   "source": [
    "You can also save your encoded text and decode it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e60460b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[890, 363, 185, 1629, 4068]\n",
      "This is a small example\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "encoded = tokenizer.encode(\"This is a small example\")\n",
    "\n",
    "# Save only the IDs (this is what models actually use)\n",
    "ids = encoded.ids\n",
    "print(ids)\n",
    "\n",
    "with open(\"encoded_text.json\", \"w\") as f:\n",
    "    json.dump(ids, f)\n",
    "\n",
    "# Reload the data\n",
    "with open(\"encoded_text.json\") as f:\n",
    "    ids = json.load(f)\n",
    "\n",
    "# And decode the text\n",
    "decoded_text = tokenizer.decode(ids)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9230a66",
   "metadata": {},
   "source": [
    "Finally, lets have a look how to load the tokenizer from an actual LLM. Lets load the GPT2 tokenizer using the Hugging Face `transformers` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c8680c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fdcdf51de704fe197af1f44ce895418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e1d64524564fa79a0d015ad5e046c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6749ab410fd4035927d8abbc532ad40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393654b814fa47e68db09ac39cbe3351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390995ce1b30453e95dff2570b992a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load pretrained GPT-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5dfbbc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['I', 'Ġam', 'Ġlearning', 'Ġabout', 'Ġtoken', 'ization', '.']\n",
      "IDs: [40, 716, 4673, 546, 11241, 1634, 13]\n",
      "Decoded:  I am learning about tokenization.\n"
     ]
    }
   ],
   "source": [
    "text = \"I am learning about tokenization.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.encode(text)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"IDs:\", ids)\n",
    "print(\"Decoded: \", tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c245d928",
   "metadata": {},
   "source": [
    "Here you see the special token Ġ. Ġ means “this token starts after a space”. This is how GPT-2 preserves word boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4d6efc",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268d7aa7",
   "metadata": {},
   "source": [
    "We continue with Lemmatization. We will use the nltk package that we first have to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56b8da6d-40df-43e4-973e-b7f3d38e3084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ivan/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/ivan/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /Users/ivan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/ivan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38741d8",
   "metadata": {},
   "source": [
    "A lemmatizer reduces a word to its dictionary base form (lemma) using linguistic knowledge such as part of speech and vocabulary. It always returns a valid word that represents the word’s underlying meaning.\n",
    "\n",
    "1.\t\"running\" → \"run\" (verb in progressive form)\n",
    "2.\t\"mice\" → \"mouse\" (irregular plural)\n",
    "3.\t\"is\" → \"be\" (infinitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00283d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0da56fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running → running\n",
      "mice → mouse\n",
      "is → is\n"
     ]
    }
   ],
   "source": [
    "# Words to lemmatize\n",
    "words = [\"running\", \"mice\", \"is\"]\n",
    "\n",
    "# Apply lemmatizer\n",
    "lemmas = [lemmatizer.lemmatize(w) for w in words]\n",
    "\n",
    "# Print results\n",
    "for word, lemma in zip(words, lemmas):\n",
    "    print(f\"{word} → {lemma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd884fb",
   "metadata": {},
   "source": [
    "Mhh, this only seemed to have worked for the word mice, what about 'running' and 'is'?\n",
    "\n",
    "We have to tell the lemmatizer which type of a word we have by providing the `pos` argument. If you leave out the `pos` parameter, the NLTK WordNetLemmatizer assumes the word is a noun by default.\n",
    "This can lead to incorrect lemmas for verbs, adjectives, or adverbs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "947dfa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running → run\n",
      "mice → mouse\n",
      "is → be\n"
     ]
    }
   ],
   "source": [
    "# Apply lemmatizer with appropriate POS\n",
    "lemmas = [\n",
    "    lemmatizer.lemmatize(\"running\", pos=\"v\"),  # verb\n",
    "    lemmatizer.lemmatize(\"mice\", pos=\"n\"),     # noun\n",
    "    lemmatizer.lemmatize(\"is\", pos=\"v\")        # verb\n",
    "]\n",
    "\n",
    "# Print results\n",
    "for word, lemma in zip(words, lemmas):\n",
    "    print(f\"{word} → {lemma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "387f3c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ivan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ivan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/ivan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ivan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/ivan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "# POS tagger (download BOTH to be safe across NLTK versions)\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50152a4e",
   "metadata": {},
   "source": [
    "You can use the following function to automatically determine the word type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fab73656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default\n",
    "\n",
    "pos_tags = nltk.pos_tag(raw_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e946a371",
   "metadata": {},
   "source": [
    "Now we lemmatize all the words in our books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce243b48",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos_tags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m lemmatized_tokens_pos \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word\u001b[38;5;241m.\u001b[39mlower(), pos\u001b[38;5;241m=\u001b[39mget_wordnet_pos(tag))\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word, tag \u001b[38;5;129;01min\u001b[39;00m pos_tags\n\u001b[1;32m      4\u001b[0m ]\n\u001b[1;32m      6\u001b[0m lemma_counter_pos \u001b[38;5;241m=\u001b[39m Counter(lemmatized_tokens_pos)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pos_tags' is not defined"
     ]
    }
   ],
   "source": [
    "lemmatized_tokens_pos = [\n",
    "    lemmatizer.lemmatize(word.lower(), pos=get_wordnet_pos(tag))\n",
    "    for word, tag in pos_tags\n",
    "]\n",
    "\n",
    "lemma_counter_pos = Counter(lemmatized_tokens_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "797f7771-4206-4e4c-b570-8c252fde76c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens after lemmatization: 66580\n",
      "dog: 69\n",
      "dogs: 0\n",
      "run: 260\n",
      "running: 6\n",
      "be: 18194\n",
      "is: 24\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique tokens after lemmatization:\", len(lemma_counter_pos))\n",
    "\n",
    "print(\"dog:\", lemma_counter_pos[\"dog\"])\n",
    "print(\"dogs:\", lemma_counter_pos[\"dogs\"])\n",
    "\n",
    "print(\"run:\", lemma_counter_pos[\"run\"])\n",
    "print(\"running:\", lemma_counter_pos[\"running\"])\n",
    "\n",
    "print(\"be:\", lemma_counter_pos[\"be\"])\n",
    "print(\"is:\", lemma_counter_pos[\"is\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b238b3",
   "metadata": {},
   "source": [
    "In a few cases the word running was apparently used as a noun rather than a verb in the context of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25344938",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171b1ff5",
   "metadata": {},
   "source": [
    "We continue with stemming. Stemming is a text-normalization technique that reduces words to a common root by mechanically removing prefixes or suffixes.\n",
    "\n",
    "1. \"connection\" → \"connect\" (removes -ion, grouping related words)\n",
    "2. \"happiness\" → \"happi\" (removes -ness, but leaves a non-word stem)\n",
    "3. \"studies\" → \"studi\" (the stemmer chops off -es, but the result is not a real word)\n",
    "\n",
    "How does it compare to a lemmatizer:\n",
    "1.\t\"running\" → \"run\" (suffix -ing is stripped)\n",
    "2.\t\"mice\" → \"mice\" (unchanged, because most stemmers don’t handle irregular forms)\n",
    "3.\t\"is\" → \"is\" (unchanged, a stemmer doesn't know about verbs infinitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbabedb-fa0f-4154-a976-e2c626ec2377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0516cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection → connect\n",
      "happiness → happi\n",
      "studies → studi\n",
      "running → run\n",
      "mice → mice\n",
      "is → is\n"
     ]
    }
   ],
   "source": [
    "# Words to lemmatize\n",
    "words = [\"connection\", \"happiness\", \"studies\", \"running\", \"mice\", \"is\"]\n",
    "\n",
    "# Apply lemmatizer\n",
    "lemmas = [stemmer.stem(w) for w in words]\n",
    "\n",
    "# Print results\n",
    "for word, lemma in zip(words, lemmas):\n",
    "    print(f\"{word} → {lemma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2440e6f9",
   "metadata": {},
   "source": [
    "We continue by stemming all words in our books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70d1dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_tokens = [stemmer.stem(t.lower()) for t in raw_tokens]\n",
    "stem_counter = Counter(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb1db3df-0e32-4cf5-bb12-5482b96b2a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens after stemming: 62330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 35087), ('of', 21489), ('and', 19744), ('to', 16121), ('a', 14892)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Unique tokens after stemming:\", len(stem_counter))\n",
    "stem_counter['dog']\n",
    "# Inspect common stems\n",
    "stem_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c97d32e",
   "metadata": {},
   "source": [
    "We observe that we have less unique tokens after stemming than after lemmatization. That can make sense, as stemming strips endings more aggressively and merges many forms into the same rough root, whereas lemmatization only combines words when they truly share the same dictionary base form."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
